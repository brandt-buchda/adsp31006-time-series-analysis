{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5103,
     "status": "ok",
     "timestamp": 1739231578150,
     "user": {
      "displayName": "Danielle Killeen",
      "userId": "01156062824676845723"
     },
     "user_tz": 360
    },
    "id": "s5WSnhRW0Yfi",
    "outputId": "681e032b-d0bb-4a72-a7e8-e14e20df0fc5"
   },
   "outputs": [],
   "source": [
    "!pip install xarray netCDF4 h5netcdf h5py requests dask --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1739231601210,
     "user": {
      "displayName": "Danielle Killeen",
      "userId": "01156062824676845723"
     },
     "user_tz": 360
    },
    "id": "fbp9DqOb0wTX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# NOAA OISST v2.1 Data URL\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr\"\n",
    "\n",
    "# Directories\n",
    "BASE_DIR = os.getcwd()  # Get current working directory inside WSL\n",
    "RAW_DIR = os.path.join(BASE_DIR, \"content\", \"oisst_raw\")\n",
    "SUBSET_DIR = os.path.join(BASE_DIR, \"content\", \"oisst_california_subset\")\n",
    "MERGED_FILE = os.path.join(BASE_DIR, \"content\", \"oisst_california_1981_2025.nc\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(SUBSET_DIR, exist_ok=True)\n",
    "\n",
    "# Define years & months to download\n",
    "YEARS = range(1981, 2026)\n",
    "MONTHS = range(1, 13)\n",
    "\n",
    "# Define California Coast region\n",
    "LAT_RANGE = slice(30, 42)  # 30¬∞N to 42¬∞N\n",
    "LON_RANGE = slice(230, 245)  # Convert -130¬∞W to -115¬∞W (360¬∞ system)\n",
    "\n",
    "# Minimum valid NetCDF file size (100 KB threshold)\n",
    "MIN_VALID_SIZE = 100_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RAW_DIR: {RAW_DIR}\")  # Debugging output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 472739,
     "status": "ok",
     "timestamp": 1739231111474,
     "user": {
      "displayName": "Danielle Killeen",
      "userId": "01156062824676845723"
     },
     "user_tz": 360
    },
    "id": "9bsUYbg609jK",
    "outputId": "33d675e6-42a0-4dc1-d021-91da0635d9c2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "for year in YEARS:\n",
    "    for month in MONTHS:\n",
    "        date_str = f\"{year}{month:02d}01\"\n",
    "        month_str = f\"{year}{month:02d}\"\n",
    "\n",
    "        file_url = f\"{BASE_URL}/{month_str}/oisst-avhrr-v02r01.{date_str}.nc\"\n",
    "        output_file = os.path.join(RAW_DIR, f\"oisst-avhrr-v02r01.{date_str}.nc\")\n",
    "\n",
    "        # Skip existing valid files\n",
    "        if os.path.exists(output_file) and os.path.getsize(output_file) > MIN_VALID_SIZE:\n",
    "            print(f\"‚úÖ Already exists: {output_file}\")\n",
    "            continue\n",
    "\n",
    "        # Check if file exists on NOAA server\n",
    "        response = requests.head(file_url)\n",
    "        if response.status_code == 404:\n",
    "            print(f\"‚ùå File not found: {file_url}\")\n",
    "            continue\n",
    "\n",
    "        # Download the file using requests\n",
    "        print(f\"üîΩ Downloading {file_url} ...\")\n",
    "        print(f\"Output file: {output_file}\")  # Debugging output\n",
    "\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "\n",
    "            # Validate download size\n",
    "            if os.path.getsize(output_file) < MIN_VALID_SIZE:\n",
    "                print(f\"‚ùå File too small, deleting: {output_file}\")\n",
    "                os.remove(output_file)\n",
    "                continue\n",
    "\n",
    "            print(f\"‚úÖ Downloaded successfully: {output_file}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Download failed: {file_url} (Status Code: {response.status_code})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/\"\n",
    "RAW_DIR = \"downloaded_files\"\n",
    "MIN_VALID_SIZE = 1024\n",
    "MAX_THREADS = 12  # Adjust this to control concurrency\n",
    "\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "def get_links(url, file_extension=None, is_directory=False):\n",
    "    \"\"\"Fetch all links from a NOAA directory webpage.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Failed to fetch: {url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    links = []\n",
    "\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if not href or href.startswith(\"..\"):  # Ignore parent directory link\n",
    "            continue\n",
    "\n",
    "        full_url = urljoin(url, href)\n",
    "\n",
    "        if is_directory and href.endswith(\"/\"):  # Subfolders\n",
    "            links.append(full_url)\n",
    "        elif file_extension and href.endswith(file_extension):  # Files\n",
    "            links.append(full_url)\n",
    "\n",
    "    return links\n",
    "\n",
    "def download_file(file_url):\n",
    "    \"\"\"Download a single file if it doesn't already exist.\"\"\"\n",
    "    filename = os.path.basename(file_url)\n",
    "    output_file = os.path.join(RAW_DIR, filename)\n",
    "\n",
    "    if os.path.exists(output_file) and os.path.getsize(output_file) > MIN_VALID_SIZE:\n",
    "        print(f\"‚úÖ Already exists: {output_file}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üîΩ Downloading {file_url} ...\")\n",
    "    response = requests.get(file_url, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "        if os.path.getsize(output_file) < MIN_VALID_SIZE:\n",
    "            print(f\"‚ùå File too small, deleting: {output_file}\")\n",
    "            os.remove(output_file)\n",
    "            return\n",
    "\n",
    "        print(f\"‚úÖ Downloaded: {output_file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Download failed: {file_url} (Status Code: {response.status_code})\")\n",
    "\n",
    "# Step 1: Get all year-month subdirectories\n",
    "subdirectories = get_links(BASE_URL, is_directory=True)\n",
    "\n",
    "# Step 2: Get all .nc file links\n",
    "all_file_links = []\n",
    "for subdir in subdirectories:\n",
    "    all_file_links.extend(get_links(subdir, file_extension=\".nc\"))\n",
    "\n",
    "# Step 3: Download files in parallel using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "    executor.map(download_file, all_file_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46498,
     "status": "ok",
     "timestamp": 1739231658763,
     "user": {
      "displayName": "Danielle Killeen",
      "userId": "01156062824676845723"
     },
     "user_tz": 360
    },
    "id": "sI7Ki8Ha2WhG",
    "outputId": "16059014-d49f-4d4a-a85d-2e7d6b2a2046"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import gc\n",
    "\n",
    "subset_files = []\n",
    "batch_size = 32  # Process in batches of 5\n",
    "\n",
    "file_list = sorted([f for f in os.listdir(RAW_DIR) if f.endswith(\".nc\")])\n",
    "\n",
    "for i in range(0, len(file_list), batch_size):\n",
    "    batch = file_list[i:i+batch_size]\n",
    "    print(f\"\\nüîπ Processing batch {i//batch_size + 1}/{len(file_list)//batch_size + 1}...\")\n",
    "\n",
    "    for file in batch:\n",
    "        file_path = os.path.join(RAW_DIR, file)\n",
    "\n",
    "        try:\n",
    "            # Open dataset with chunking to reduce memory use\n",
    "            ds = xr.open_dataset(file_path, engine=\"netcdf4\", chunks={\"time\": 1})\n",
    "\n",
    "            # Subset region (California Coast)\n",
    "            ds_subset = ds.sel(lat=LAT_RANGE, lon=LON_RANGE)\n",
    "\n",
    "            # Keep only SST variable\n",
    "            ds_subset = ds_subset[[\"sst\"]]\n",
    "\n",
    "            # Save subset file\n",
    "            subset_file_path = os.path.join(SUBSET_DIR, file)\n",
    "            ds_subset.to_netcdf(subset_file_path)\n",
    "\n",
    "            # Close dataset to free memory\n",
    "            ds_subset.close()\n",
    "            ds.close()\n",
    "\n",
    "            subset_files.append(subset_file_path)\n",
    "            print(f\"‚úÖ Processed: {file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Skipping corrupt file: {file_path} | Error: {e}\")\n",
    "\n",
    "    # Manually clear memory after each batch\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ All {len(subset_files)} files saved in {SUBSET_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4147,
     "status": "ok",
     "timestamp": 1739231666788,
     "user": {
      "displayName": "Danielle Killeen",
      "userId": "01156062824676845723"
     },
     "user_tz": 360
    },
    "id": "YgnMCD8T2nUt",
    "outputId": "ccefdaab-e1c1-4f48-c67b-6d3dc724021f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "subset_output_dir = \"/content/oisst_california_subset\"\n",
    "corrupt_files = []\n",
    "\n",
    "# Check all NetCDF files\n",
    "for file in sorted(os.listdir(subset_output_dir)):\n",
    "    if file.endswith(\".nc\"):\n",
    "        file_path = os.path.join(subset_output_dir, file)\n",
    "        try:\n",
    "            ds = xr.open_dataset(file_path, engine=\"netcdf4\")\n",
    "            ds.close()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Corrupt file detected: {file_path} | Error: {e}\")\n",
    "            corrupt_files.append(file_path)\n",
    "\n",
    "# Remove corrupted files\n",
    "if corrupt_files:\n",
    "    print(f\"\\nüö® Removing {len(corrupt_files)} corrupt files...\")\n",
    "    for bad_file in corrupt_files:\n",
    "        os.remove(bad_file)\n",
    "    print(\"‚úÖ Corrupt files deleted.\")\n",
    "else:\n",
    "    print(\"‚úÖ No corrupt files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1739231669777,
     "user": {
      "displayName": "Danielle Killeen",
      "userId": "01156062824676845723"
     },
     "user_tz": 360
    },
    "id": "PU9ye9pC3PCZ",
    "outputId": "21431d54-a90a-41e3-cac7-40f873ffd66a"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Pick two files to compare metadata\n",
    "file1 = \"./content/oisst_california_subset/oisst-avhrr-v02r01.20100501.nc\"\n",
    "file2 = \"./content/oisst_california_subset/oisst-avhrr-v02r01.20100601.nc\"\n",
    "\n",
    "# Open files\n",
    "ds1 = xr.open_dataset(file1, engine=\"netcdf4\")\n",
    "ds2 = xr.open_dataset(file2, engine=\"netcdf4\")\n",
    "\n",
    "# Compare dimensions\n",
    "print(\"\\nüîπ Dimensions in File 1:\")\n",
    "print(ds1.dims)\n",
    "print(\"\\nüîπ Dimensions in File 2:\")\n",
    "print(ds2.dims)\n",
    "\n",
    "# Compare coordinate variables\n",
    "print(\"\\nüîπ Coordinates in File 1:\")\n",
    "print(ds1.coords)\n",
    "print(\"\\nüîπ Coordinates in File 2:\")\n",
    "print(ds2.coords)\n",
    "\n",
    "# Compare attributes\n",
    "print(\"\\nüîπ Global attributes in File 1:\")\n",
    "print(ds1.attrs)\n",
    "print(\"\\nüîπ Global attributes in File 2:\")\n",
    "print(ds2.attrs)\n",
    "\n",
    "# Close datasets\n",
    "ds1.close()\n",
    "ds2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1739231678068,
     "user": {
      "displayName": "Danielle Killeen",
      "userId": "01156062824676845723"
     },
     "user_tz": 360
    },
    "id": "QJk9tV0a3g9_",
    "outputId": "e9e26209-e6ac-4c8f-8c3c-ed7b88e88250"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "subset_output_dir = \"./content/oisst_california_subset\"\n",
    "nc_files = sorted([\n",
    "    os.path.join(subset_output_dir, f) for f in os.listdir(subset_output_dir) if f.endswith(\".nc\")\n",
    "])\n",
    "\n",
    "# Check time variable for each file\n",
    "for file in nc_files[:10]:  # Check first 10 files\n",
    "    ds = xr.open_dataset(file, engine=\"netcdf4\")\n",
    "    print(f\"\\nüîπ {file}\")\n",
    "    print(ds.time)\n",
    "    ds.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1739231684876,
     "user": {
      "displayName": "Danielle Killeen",
      "userId": "01156062824676845723"
     },
     "user_tz": 360
    },
    "id": "_9J4iizf3uIZ",
    "outputId": "310ec6a1-0536-408f-cf34-4625d92f7fa1"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Select a few files for testing\n",
    "test_files = [\n",
    "    \"./content/oisst_california_subset/oisst-avhrr-v02r01.20100501.nc\",\n",
    "    \"./content/oisst_california_subset/oisst-avhrr-v02r01.20100601.nc\",\n",
    "    \"./content/oisst_california_subset/oisst-avhrr-v02r01.20100701.nc\",\n",
    "    \"./content/oisst_california_subset/oisst-avhrr-v02r01.20100801.nc\",\n",
    "    \"./content/oisst_california_subset/oisst-avhrr-v02r01.20100901.nc\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    ds_list = [xr.open_dataset(f, engine=\"netcdf4\") for f in test_files]\n",
    "    ds_combined = xr.concat(ds_list, dim=\"time\")  # Merge along time axis\n",
    "\n",
    "    print(\"‚úÖ Test merge successful!\")\n",
    "\n",
    "    # Close datasets\n",
    "    for ds in ds_list:\n",
    "        ds.close()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Merge failed! Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4305,
     "status": "ok",
     "timestamp": 1739231706137,
     "user": {
      "displayName": "Danielle Killeen",
      "userId": "01156062824676845723"
     },
     "user_tz": 360
    },
    "id": "yquVz31q5S-u",
    "outputId": "d2e0e596-84bb-4c4a-9e6a-00cd95709355"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y nco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1739231713176,
     "user": {
      "displayName": "Danielle Killeen",
      "userId": "01156062824676845723"
     },
     "user_tz": 360
    },
    "id": "FFKc_fu35hdH",
    "outputId": "ae0df53e-6cfa-4bd6-b4f5-94d507f4eb9c"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run the 'pwd' command to print the current working directory of the subprocess\n",
    "result = subprocess.run('cd /mnt/c/ & pwd', shell=True, capture_output=True, text=True)\n",
    "\n",
    "# Print the result\n",
    "print(\"Subprocess working directory:\", result.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyperclip\n",
    "import subprocess\n",
    "\n",
    "# Run the 'pwd' command to print the current working directory of the subprocess\n",
    "result = subprocess.run(\n",
    "    \"pwd\",\n",
    "    shell=True,  # Use shell to execute the command\n",
    "    check=True,  # Raise an error if the command fails\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(f\"Subprocess working directory: {result.stdout.decode().strip()}\")\n",
    "\n",
    "subset_output_dir = r\"./content/oisst_california_subset/\"\n",
    "final_output_file = r\"./content/oisst_california_1981_2025.nc\"\n",
    "\n",
    "# Get sorted list of valid NetCDF files\n",
    "valid_files = sorted([\n",
    "    os.path.join(subset_output_dir, f) for f in os.listdir(subset_output_dir) if f.endswith(\".nc\")\n",
    "])\n",
    "\n",
    "# Define the batch size (adjust as needed)\n",
    "batch_size = 100  # Process 100 files at a time, or adjust to your system's capacity\n",
    "num_batches = (len(valid_files) // batch_size) + 1\n",
    "\n",
    "# Intermediate output files for batch merging\n",
    "temp_output_file = r\"./content/temp_batch_merged.nc\"\n",
    "\n",
    "# Process files in batches\n",
    "for i in range(num_batches):\n",
    "    # Create a batch of files for the current batch\n",
    "    batch_files = valid_files[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "    # Generate merge command for the batch\n",
    "    merge_command = [\"ncrcat\"] + batch_files + [temp_output_file]\n",
    "\n",
    "    # Copy the command to clipboard\n",
    "    pyperclip.copy(\" \".join(merge_command))\n",
    "\n",
    "    print(f\"\\nüîπ Merging batch {i + 1}/{num_batches} of {len(valid_files)} files...\")\n",
    "    print(f\"‚úîÔ∏è Merge command copied to clipboard for batch {i + 1}: \\n{' '.join(merge_command)}\")\n",
    "\n",
    "    try:\n",
    "        # Execute the batch merge command using subprocess\n",
    "        result = subprocess.run(merge_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(f\"‚úÖ Batch {i + 1} merged successfully.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # Handle error if the command fails\n",
    "        print(f\"‚ùå Error: Command failed for batch {i + 1}\")\n",
    "        print(f\"Error message: {e.stderr.decode()}\")\n",
    "        break\n",
    "\n",
    "    # After each batch, append to the final output (or merge the batch incrementally)\n",
    "    if i == 0:\n",
    "        # For the first batch, use temp output file as the final result\n",
    "        os.rename(temp_output_file, final_output_file)\n",
    "    else:\n",
    "        # For subsequent batches, append the merged result to the final output\n",
    "        append_command = [\"ncrcat\", final_output_file, temp_output_file, final_output_file]\n",
    "\n",
    "        try:\n",
    "            # Execute the append command using subprocess\n",
    "            result = subprocess.run(append_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"‚úÖ Batch {i + 1} appended to final dataset.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Error: Command failed while appending batch {i + 1} to the final output.\")\n",
    "            print(f\"Error message: {e.stderr.decode()}\")\n",
    "            break\n",
    "\n",
    "print(f\"‚úÖ Final dataset saved as {final_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directories and files\n",
    "subset_output_dir = \"C:\\\\Users\\\\Brandt\\\\Documents\\\\UniversityOfChicago\\\\Courses\\\\31006_TimeSeriesAnalysis\\\\project\\\\data\\\\content\\\\oisst_california_subset\\\\\"\n",
    "final_output_file = \"C:\\\\Users\\\\Brandt\\\\Documents\\\\UniversityOfChicago\\\\Courses\\\\31006_TimeSeriesAnalysis\\\\project\\\\data\\\\content\\\\oisst_california_1981_2025.nc\"\n",
    "\n",
    "# Get sorted list of valid NetCDF files\n",
    "valid_files = sorted([\n",
    "    os.path.join(subset_output_dir, f) for f in os.listdir(subset_output_dir) if f.endswith(\".nc\")\n",
    "])\n",
    "\n",
    "# Define the batch size (adjust as needed)\n",
    "batch_size = 100  # Process 100 files at a time, or adjust to your system's capacity\n",
    "num_batches = (len(valid_files) // batch_size) + 1\n",
    "\n",
    "# Temporary output file for batch merging\n",
    "temp_output_file = os.path.join(subset_output_dir, \"temp_batch_merged.nc\")\n",
    "\n",
    "# Process files in batches\n",
    "for i in range(num_batches):\n",
    "    # Create a batch of files for the current batch\n",
    "    batch_files = valid_files[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "    # Generate merge command for the batch\n",
    "    merge_command = f\"ncrcat {' '.join(batch_files)} {temp_output_file}\"\n",
    "\n",
    "    print(f\"\\nüîπ Merging batch {i + 1}/{num_batches} of {len(valid_files)} files...\")\n",
    "    print(f\"‚úîÔ∏è Merge command copied to clipboard for batch {i + 1}: \\n{merge_command}\")\n",
    "\n",
    "    # Execute the batch merge command\n",
    "    result = os.system(merge_command)\n",
    "    if result != 0:\n",
    "        print(f\"‚ùå Error: Command failed for batch {i + 1}\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"‚úÖ Batch {i + 1} merged successfully.\")\n",
    "\n",
    "    # After each batch, append to the final output (or merge the batch incrementally)\n",
    "    if i == 0:\n",
    "        # For the first batch, use temp output file as the final result\n",
    "        os.rename(temp_output_file, final_output_file)\n",
    "    else:\n",
    "        # For subsequent batches, append the merged result to the final output\n",
    "        append_command = f\"ncrcat {final_output_file} {temp_output_file} {final_output_file}\"\n",
    "        result = os.system(append_command)\n",
    "        if result != 0:\n",
    "            print(f\"‚ùå Error: Command failed while appending batch {i + 1} to the final output.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"‚úÖ Batch {i + 1} appended to final dataset.\")\n",
    "\n",
    "# Clean up temporary files\n",
    "if os.path.exists(temp_output_file):\n",
    "    os.remove(temp_output_file)\n",
    "\n",
    "print(f\"‚úÖ Final dataset saved as {final_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 865,
     "status": "ok",
     "timestamp": 1739231716872,
     "user": {
      "displayName": "Danielle Killeen",
      "userId": "01156062824676845723"
     },
     "user_tz": 360
    },
    "id": "NR8CwIVe5olM",
    "outputId": "6bf3902a-03c2-4ea7-d3c8-29d122c93175"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Move the final dataset\n",
    "!mv /content/oisst_california_1981_2025.nc /content/drive/MyDrive/oisst_california_1981_2025.nc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnUtZ5SN6fPA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMvKGwWXushi8MNqrtJxvFo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
